import streamlit as st

def dl_general():
    st.image('./images/dl/main.png')
    general="""
            1. Loading Dataset
            2. Chose Model (including activation/relu/softmax): `model = keras.Sequential([layers.Dense(512, activation="relu"),layers.Dense(10, activation="softmax")])`
            3. Compile (optimizer, loss function, metric)
            4. Reshape preprocessing data.
            5. 
            
            
            
            Deep learning is a technique used to make predictions using data, and it heavily relies on neural networks. 
            
            Deep learning framework like **TensorFlow** or **PyTorch** instead of building your own neural network. 
            That said, having some knowledge of how neural networks work is helpful because you can use it to better architect your deep learning models.

            **Traditional Machine Learning:**

            - These models typically involve feature engineering, where domain-specific features are manually crafted from raw data to feed into the learning algorithm.
            - Examples of traditional machine learning algorithms include linear regression, logistic regression, decision trees, support vector machines, and k-nearest neighbors, among others.
            - While some traditional machine learning algorithms may use ensemble techniques that combine multiple models (e.g., random forests, gradient boosting), they are not typically referred to as "multi-layer" in the same sense as deep neural networks.

            **Deep Learning:**

            Deep learning, on the other hand, specifically refers to neural networks with multiple layers (hence the term "deep").
            - Deep learning architectures consist of multiple layers of interconnected neurons, allowing them to learn complex representations and hierarchies of features directly from raw data.
            - Deep learning models are capable of automatically learning feature representations from data without requiring explicit feature engineering.
            - Examples of deep learning architectures include convolutional neural networks (CNNs) for image analysis, recurrent neural networks (RNNs) for sequential data, and transformer-based architectures for natural language processing.
            - The depth of neural networks in deep learning refers to the number of layers, and deep networks may consist of dozens or even hundreds of layers.
            
            
        """
    st.markdown(general)    
    st.info("The Exchange Of Methods And Algorithms Between Human And Machine To Deep Learn And Apply Problem Solving Is Known As Deep Learning (DL) â€• P.S. Jagadeesh Kumar")


def dl_theory():
    st.header("ğŸ§ 1. Long Short-Term Memory networks")
    st.markdown("""

        LSTM networks are a type of Recurrent Neural Network (RNN) specially designed to remember and process sequences of data over long periods. 
        What sets LSTMs apart from traditional RNNs is their ability to preserve information for long durations, courtesy of their unique structure comprising three gates: the input, forget, and output gates.
        These gates collaboratively manage the flow of information, deciding what to retain and what to discard, thereby mitigating the issue of vanishing gradients â€” a common problem in standard RNNs.
    
    """)
    st.image("./images/lstm.png")
    
    st.header("ğŸ‘©â€ğŸ«2. Attention Mechanism")
    st.markdown("""
        The attention mechanism, initially popularized in the field of natural language processing, has found its way into various other domains, including finance. 
        It operates on a simple yet profound concept: not all parts of the input sequence are equally important. 
        By allowing the model to focus on specific parts of the input sequence while ignoring others, the attention mechanism enhances the modelâ€™s context understanding capabilities.

        Incorporating attention into LSTM networks results in a more focused and context-aware model. 
        When predicting stock prices, certain historical data points may be more relevant than others. 
        The attention mechanism empowers the LSTM to weigh these points more heavily, leading to more accurate and nuanced predictions.

        tensorflowä¸¤ç§attentionæœºåˆ¶ï¼Œåˆ†åˆ«ä¸ºBahdanau attentionï¼Œå’ŒLuongAttention.
        Attention è§£å†³äº† RNN ä¸èƒ½å¹¶è¡Œè®¡ç®—çš„é—®é¢˜ã€‚Attentionæœºåˆ¶æ¯ä¸€æ­¥è®¡ç®—ä¸ä¾èµ–äºä¸Šä¸€æ­¥çš„è®¡ç®—ç»“æœï¼Œå› æ­¤å¯ä»¥å’ŒCNNä¸€æ ·å¹¶è¡Œå¤„ç†ã€‚
        æ¨¡å‹å¤æ‚åº¦è·Ÿ CNNã€RNN ç›¸æ¯”ï¼Œå¤æ‚åº¦æ›´å°ï¼Œå‚æ•°ä¹Ÿæ›´å°‘ã€‚æ‰€ä»¥å¯¹ç®—åŠ›çš„è¦æ±‚ä¹Ÿå°±æ›´å°ã€‚
        åœ¨ Attention æœºåˆ¶å¼•å…¥ä¹‹å‰ï¼Œæœ‰ä¸€ä¸ªé—®é¢˜å¤§å®¶ä¸€ç›´å¾ˆè‹¦æ¼ï¼šé•¿è·ç¦»çš„ä¿¡æ¯ä¼šè¢«å¼±åŒ–ï¼Œå°±å¥½åƒè®°å¿†èƒ½åŠ›å¼±çš„äººï¼Œè®°ä¸ä½è¿‡å»çš„äº‹æƒ…æ˜¯ä¸€æ ·çš„ã€‚

        Attention æ˜¯æŒ‘é‡ç‚¹ï¼Œå°±ç®—æ–‡æœ¬æ¯”è¾ƒé•¿ï¼Œä¹Ÿèƒ½ä»ä¸­é—´æŠ“ä½é‡ç‚¹ï¼Œä¸ä¸¢å¤±é‡è¦çš„ä¿¡æ¯ã€‚ä¸‹å›¾çº¢è‰²çš„é¢„æœŸå°±æ˜¯è¢«æŒ‘å‡ºæ¥çš„é‡ç‚¹ã€‚

        Attention ç»å¸¸ä¼šå’Œ Encoderâ€“Decoder ä¸€èµ·è¯´ï¼Œä¹‹å‰çš„æ–‡ç« ã€Šä¸€æ–‡çœ‹æ‡‚ NLP é‡Œçš„æ¨¡å‹æ¡†æ¶ Encoder-Decoder å’Œ Seq2Seqã€‹ ä¹Ÿæåˆ°äº† Attentionã€‚
    """)
    st.image("./images/attention.gif")
    st.header("Attention åŸç†çš„3æ­¥åˆ†è§£ï¼š")
    st.image("./images/attentionpipeline.png")
    st.markdown("""

        ç¬¬ä¸€æ­¥ï¼š query å’Œ key è¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—ï¼Œå¾—åˆ°æƒå€¼

        ç¬¬äºŒæ­¥ï¼šå°†æƒå€¼è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¾—åˆ°ç›´æ¥å¯ç”¨çš„æƒé‡

        ç¬¬ä¸‰æ­¥ï¼šå°†æƒé‡å’Œ value è¿›è¡ŒåŠ æƒæ±‚å’Œ

        ä»ä¸Šé¢çš„å»ºæ¨¡ï¼Œæˆ‘ä»¬å¯ä»¥å¤§è‡´æ„Ÿå—åˆ° Attention çš„æ€è·¯ç®€å•ï¼Œå››ä¸ªå­—â€œå¸¦æƒæ±‚å’Œâ€å°±å¯ä»¥é«˜åº¦æ¦‚æ‹¬ï¼Œå¤§é“è‡³ç®€ã€‚åšä¸ªä¸å¤ªæ°å½“çš„ç±»æ¯”ï¼Œäººç±»å­¦ä¹ ä¸€é—¨æ–°è¯­è¨€åŸºæœ¬ç»å†å››ä¸ªé˜¶æ®µï¼šæ­»è®°ç¡¬èƒŒï¼ˆé€šè¿‡é˜…è¯»èƒŒè¯µå­¦ä¹ è¯­æ³•ç»ƒä¹ è¯­æ„Ÿï¼‰->æçº²æŒˆé¢†ï¼ˆç®€å•å¯¹è¯é å¬æ‡‚å¥å­ä¸­çš„å…³é”®è¯æ±‡å‡†ç¡®ç†è§£æ ¸å¿ƒæ„æ€ï¼‰->èä¼šè´¯é€šï¼ˆå¤æ‚å¯¹è¯æ‡‚å¾—ä¸Šä¸‹æ–‡æŒ‡ä»£ã€è¯­è¨€èƒŒåçš„è”ç³»ï¼Œå…·å¤‡äº†ä¸¾ä¸€åä¸‰çš„å­¦ä¹ èƒ½åŠ›ï¼‰->ç™»å³°é€ æï¼ˆæ²‰æµ¸åœ°å¤§é‡ç»ƒä¹ ï¼‰ã€‚

        è¿™ä¹Ÿå¦‚åŒattentionçš„å‘å±•è„‰ç»œï¼ŒRNN æ—¶ä»£æ˜¯æ­»è®°ç¡¬èƒŒçš„æ—¶æœŸï¼Œattention çš„æ¨¡å‹å­¦ä¼šäº†æçº²æŒˆé¢†ï¼Œè¿›åŒ–åˆ° transformerï¼Œèæ±‡è´¯é€šï¼Œå…·å¤‡ä¼˜ç§€çš„è¡¨è¾¾å­¦ä¹ èƒ½åŠ›ï¼Œå†åˆ° GPTã€BERTï¼Œé€šè¿‡å¤šä»»åŠ¡å¤§è§„æ¨¡å­¦ä¹ ç§¯ç´¯å®æˆ˜ç»éªŒï¼Œæˆ˜æ–—åŠ›çˆ†æ£šã€‚

        è¦å›ç­”ä¸ºä»€ä¹ˆ attention è¿™ä¹ˆä¼˜ç§€ï¼Ÿæ˜¯å› ä¸ºå®ƒè®©æ¨¡å‹å¼€çªäº†ï¼Œæ‡‚å¾—äº†æçº²æŒˆé¢†ï¼Œå­¦ä¼šäº†èä¼šè´¯é€šã€‚

        **Attention çš„ N ç§ç±»å‹**
        Attention æœ‰å¾ˆå¤šç§ä¸åŒçš„ç±»å‹ï¼šSoft Attentionã€Hard Attentionã€é™æ€Attentionã€åŠ¨æ€Attentionã€Self Attention ç­‰ç­‰ã€‚ä¸‹é¢å°±è·Ÿå¤§å®¶è§£é‡Šä¸€ä¸‹è¿™äº›ä¸åŒçš„ Attention éƒ½æœ‰å“ªäº›å·®åˆ«ã€‚

        1. è®¡ç®—åŒºåŸŸ

        æ ¹æ®Attentionçš„è®¡ç®—åŒºåŸŸï¼Œå¯ä»¥åˆ†æˆä»¥ä¸‹å‡ ç§ï¼š

        1ï¼‰Soft Attentionï¼Œè¿™æ˜¯æ¯”è¾ƒå¸¸è§çš„Attentionæ–¹å¼ï¼Œå¯¹æ‰€æœ‰keyæ±‚æƒé‡æ¦‚ç‡ï¼Œæ¯ä¸ªkeyéƒ½æœ‰ä¸€ä¸ªå¯¹åº”çš„æƒé‡ï¼Œæ˜¯ä¸€ç§å…¨å±€çš„è®¡ç®—æ–¹å¼ï¼ˆä¹Ÿå¯ä»¥å«Global Attentionï¼‰ã€‚è¿™ç§æ–¹å¼æ¯”è¾ƒç†æ€§ï¼Œå‚è€ƒäº†æ‰€æœ‰keyçš„å†…å®¹ï¼Œå†è¿›è¡ŒåŠ æƒã€‚ä½†æ˜¯è®¡ç®—é‡å¯èƒ½ä¼šæ¯”è¾ƒå¤§ä¸€äº›ã€‚

        2ï¼‰Hard Attentionï¼Œè¿™ç§æ–¹å¼æ˜¯ç›´æ¥ç²¾å‡†å®šä½åˆ°æŸä¸ªkeyï¼Œå…¶ä½™keyå°±éƒ½ä¸ç®¡äº†ï¼Œç›¸å½“äºè¿™ä¸ªkeyçš„æ¦‚ç‡æ˜¯1ï¼Œå…¶ä½™keyçš„æ¦‚ç‡å…¨éƒ¨æ˜¯0ã€‚å› æ­¤è¿™ç§å¯¹é½æ–¹å¼è¦æ±‚å¾ˆé«˜ï¼Œè¦æ±‚ä¸€æ­¥åˆ°ä½ï¼Œå¦‚æœæ²¡æœ‰æ­£ç¡®å¯¹é½ï¼Œä¼šå¸¦æ¥å¾ˆå¤§çš„å½±å“ã€‚å¦ä¸€æ–¹é¢ï¼Œå› ä¸ºä¸å¯å¯¼ï¼Œä¸€èˆ¬éœ€è¦ç”¨å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•è¿›è¡Œè®­ç»ƒã€‚ï¼ˆæˆ–è€…ä½¿ç”¨gumbel softmaxä¹‹ç±»çš„ï¼‰

        3ï¼‰Local Attentionï¼Œè¿™ç§æ–¹å¼å…¶å®æ˜¯ä»¥ä¸Šä¸¤ç§æ–¹å¼çš„ä¸€ä¸ªæŠ˜ä¸­ï¼Œå¯¹ä¸€ä¸ªçª—å£åŒºåŸŸè¿›è¡Œè®¡ç®—ã€‚å…ˆç”¨Hardæ–¹å¼å®šä½åˆ°æŸä¸ªåœ°æ–¹ï¼Œä»¥è¿™ä¸ªç‚¹ä¸ºä¸­å¿ƒå¯ä»¥å¾—åˆ°ä¸€ä¸ªçª—å£åŒºåŸŸï¼Œåœ¨è¿™ä¸ªå°åŒºåŸŸå†…ç”¨Softæ–¹å¼æ¥ç®—Attentionã€‚

        2. æ‰€ç”¨ä¿¡æ¯

        å‡è®¾æˆ‘ä»¬è¦å¯¹ä¸€æ®µåŸæ–‡è®¡ç®—Attentionï¼Œè¿™é‡ŒåŸæ–‡æŒ‡çš„æ˜¯æˆ‘ä»¬è¦åšattentionçš„æ–‡æœ¬ï¼Œé‚£ä¹ˆæ‰€ç”¨ä¿¡æ¯åŒ…æ‹¬å†…éƒ¨ä¿¡æ¯å’Œå¤–éƒ¨ä¿¡æ¯ï¼Œå†…éƒ¨ä¿¡æ¯æŒ‡çš„æ˜¯åŸæ–‡æœ¬èº«çš„ä¿¡æ¯ï¼Œè€Œå¤–éƒ¨ä¿¡æ¯æŒ‡çš„æ˜¯é™¤åŸæ–‡ä»¥å¤–çš„é¢å¤–ä¿¡æ¯ã€‚

        1ï¼‰General Attentionï¼Œè¿™ç§æ–¹å¼åˆ©ç”¨åˆ°äº†å¤–éƒ¨ä¿¡æ¯ï¼Œå¸¸ç”¨äºéœ€è¦æ„å»ºä¸¤æ®µæ–‡æœ¬å…³ç³»çš„ä»»åŠ¡ï¼Œqueryä¸€èˆ¬åŒ…å«äº†é¢å¤–ä¿¡æ¯ï¼Œæ ¹æ®å¤–éƒ¨queryå¯¹åŸæ–‡è¿›è¡Œå¯¹é½ã€‚

        æ¯”å¦‚åœ¨é˜…è¯»ç†è§£ä»»åŠ¡ä¸­ï¼Œéœ€è¦æ„å»ºé—®é¢˜å’Œæ–‡ç« çš„å…³è”ï¼Œå‡è®¾ç°åœ¨baselineæ˜¯ï¼Œå¯¹é—®é¢˜è®¡ç®—å‡ºä¸€ä¸ªé—®é¢˜å‘é‡qï¼ŒæŠŠè¿™ä¸ªqå’Œæ‰€æœ‰çš„æ–‡ç« è¯å‘é‡æ‹¼æ¥èµ·æ¥ï¼Œè¾“å…¥åˆ°LSTMä¸­è¿›è¡Œå»ºæ¨¡ã€‚é‚£ä¹ˆåœ¨è¿™ä¸ªæ¨¡å‹ä¸­ï¼Œæ–‡ç« æ‰€æœ‰è¯å‘é‡å…±äº«åŒä¸€ä¸ªé—®é¢˜å‘é‡ï¼Œç°åœ¨æˆ‘ä»¬æƒ³è®©æ–‡ç« æ¯ä¸€æ­¥çš„è¯å‘é‡éƒ½æœ‰ä¸€ä¸ªä¸åŒçš„é—®é¢˜å‘é‡ï¼Œä¹Ÿå°±æ˜¯ï¼Œåœ¨æ¯ä¸€æ­¥ä½¿ç”¨æ–‡ç« åœ¨è¯¥æ­¥ä¸‹çš„è¯å‘é‡å¯¹é—®é¢˜æ¥ç®—attentionï¼Œè¿™é‡Œé—®é¢˜å±äºåŸæ–‡ï¼Œæ–‡ç« è¯å‘é‡å°±å±äºå¤–éƒ¨ä¿¡æ¯ã€‚

        2ï¼‰Local Attentionï¼Œè¿™ç§æ–¹å¼åªä½¿ç”¨å†…éƒ¨ä¿¡æ¯ï¼Œkeyå’Œvalueä»¥åŠqueryåªå’Œè¾“å…¥åŸæ–‡æœ‰å…³ï¼Œåœ¨self attentionä¸­ï¼Œkey=value=queryã€‚æ—¢ç„¶æ²¡æœ‰å¤–éƒ¨ä¿¡æ¯ï¼Œé‚£ä¹ˆåœ¨åŸæ–‡ä¸­çš„æ¯ä¸ªè¯å¯ä»¥è·Ÿè¯¥å¥å­ä¸­çš„æ‰€æœ‰è¯è¿›è¡ŒAttentionè®¡ç®—ï¼Œç›¸å½“äºå¯»æ‰¾åŸæ–‡å†…éƒ¨çš„å…³ç³»ã€‚

        è¿˜æ˜¯ä¸¾é˜…è¯»ç†è§£ä»»åŠ¡çš„ä¾‹å­ï¼Œä¸Šé¢çš„baselineä¸­æåˆ°ï¼Œå¯¹é—®é¢˜è®¡ç®—å‡ºä¸€ä¸ªå‘é‡qï¼Œé‚£ä¹ˆè¿™é‡Œä¹Ÿå¯ä»¥ç”¨ä¸Šattentionï¼Œåªç”¨é—®é¢˜è‡ªèº«çš„ä¿¡æ¯å»åšattentionï¼Œè€Œä¸å¼•å…¥æ–‡ç« ä¿¡æ¯ã€‚

        3. ç»“æ„å±‚æ¬¡

        ç»“æ„æ–¹é¢æ ¹æ®æ˜¯å¦åˆ’åˆ†å±‚æ¬¡å…³ç³»ï¼Œåˆ†ä¸ºå•å±‚attentionï¼Œå¤šå±‚attentionå’Œå¤šå¤´attentionï¼š

        1ï¼‰å•å±‚Attentionï¼Œè¿™æ˜¯æ¯”è¾ƒæ™®éçš„åšæ³•ï¼Œç”¨ä¸€ä¸ªqueryå¯¹ä¸€æ®µåŸæ–‡è¿›è¡Œä¸€æ¬¡attentionã€‚

        2ï¼‰å¤šå±‚Attentionï¼Œä¸€èˆ¬ç”¨äºæ–‡æœ¬å…·æœ‰å±‚æ¬¡å…³ç³»çš„æ¨¡å‹ï¼Œå‡è®¾æˆ‘ä»¬æŠŠä¸€ä¸ªdocumentåˆ’åˆ†æˆå¤šä¸ªå¥å­ï¼Œåœ¨ç¬¬ä¸€å±‚ï¼Œæˆ‘ä»¬åˆ†åˆ«å¯¹æ¯ä¸ªå¥å­ä½¿ç”¨attentionè®¡ç®—å‡ºä¸€ä¸ªå¥å‘é‡ï¼ˆä¹Ÿå°±æ˜¯å•å±‚attentionï¼‰ï¼›åœ¨ç¬¬äºŒå±‚ï¼Œæˆ‘ä»¬å¯¹æ‰€æœ‰å¥å‘é‡å†åšattentionè®¡ç®—å‡ºä¸€ä¸ªæ–‡æ¡£å‘é‡ï¼ˆä¹Ÿæ˜¯ä¸€ä¸ªå•å±‚attentionï¼‰ï¼Œæœ€åå†ç”¨è¿™ä¸ªæ–‡æ¡£å‘é‡å»åšä»»åŠ¡ã€‚

        3ï¼‰å¤šå¤´Attentionï¼Œè¿™æ˜¯Attention is All You Needä¸­æåˆ°çš„multi-head attentionï¼Œç”¨åˆ°äº†å¤šä¸ªqueryå¯¹ä¸€æ®µåŸæ–‡è¿›è¡Œäº†å¤šæ¬¡attentionï¼Œæ¯ä¸ªqueryéƒ½å…³æ³¨åˆ°åŸæ–‡çš„ä¸åŒéƒ¨åˆ†ï¼Œç›¸å½“äºé‡å¤åšå¤šæ¬¡å•å±‚attentionï¼š


        æœ€åå†æŠŠè¿™äº›ç»“æœæ‹¼æ¥èµ·æ¥ï¼š


        4. æ¨¡å‹æ–¹é¢

        ä»æ¨¡å‹ä¸Šçœ‹ï¼ŒAttentionä¸€èˆ¬ç”¨åœ¨CNNå’ŒLSTMä¸Šï¼Œä¹Ÿå¯ä»¥ç›´æ¥è¿›è¡Œçº¯Attentionè®¡ç®—ã€‚

        1ï¼‰CNN+Attention

        CNNçš„å·ç§¯æ“ä½œå¯ä»¥æå–é‡è¦ç‰¹å¾ï¼Œæˆ‘è§‰å¾—è¿™ä¹Ÿç®—æ˜¯Attentionçš„æ€æƒ³ï¼Œä½†æ˜¯CNNçš„å·ç§¯æ„Ÿå—è§†é‡æ˜¯å±€éƒ¨çš„ï¼Œéœ€è¦é€šè¿‡å åŠ å¤šå±‚å·ç§¯åŒºå»æ‰©å¤§è§†é‡ã€‚å¦å¤–ï¼ŒMax Poolingç›´æ¥æå–æ•°å€¼æœ€å¤§çš„ç‰¹å¾ï¼Œä¹Ÿåƒæ˜¯hard attentionçš„æ€æƒ³ï¼Œç›´æ¥é€‰ä¸­æŸä¸ªç‰¹å¾ã€‚

        CNNä¸ŠåŠ Attentionå¯ä»¥åŠ åœ¨è¿™å‡ æ–¹é¢ï¼š

        a. åœ¨å·ç§¯æ“ä½œå‰åšattentionï¼Œæ¯”å¦‚Attention-Based BCNN-1ï¼Œè¿™ä¸ªä»»åŠ¡æ˜¯æ–‡æœ¬è•´å«ä»»åŠ¡éœ€è¦å¤„ç†ä¸¤æ®µæ–‡æœ¬ï¼ŒåŒæ—¶å¯¹ä¸¤æ®µè¾“å…¥çš„åºåˆ—å‘é‡è¿›è¡Œattentionï¼Œè®¡ç®—å‡ºç‰¹å¾å‘é‡ï¼Œå†æ‹¼æ¥åˆ°åŸå§‹å‘é‡ä¸­ï¼Œä½œä¸ºå·ç§¯å±‚çš„è¾“å…¥ã€‚

        b. åœ¨å·ç§¯æ“ä½œååšattentionï¼Œæ¯”å¦‚Attention-Based BCNN-2ï¼Œå¯¹ä¸¤æ®µæ–‡æœ¬çš„å·ç§¯å±‚çš„è¾“å‡ºåšattentionï¼Œä½œä¸ºpoolingå±‚çš„è¾“å…¥ã€‚

        c. åœ¨poolingå±‚åšattentionï¼Œä»£æ›¿max poolingã€‚æ¯”å¦‚Attention poolingï¼Œé¦–å…ˆæˆ‘ä»¬ç”¨LSTMå­¦åˆ°ä¸€ä¸ªæ¯”è¾ƒå¥½çš„å¥å‘é‡ï¼Œä½œä¸ºqueryï¼Œç„¶åç”¨CNNå…ˆå­¦ä¹ åˆ°ä¸€ä¸ªç‰¹å¾çŸ©é˜µä½œä¸ºkeyï¼Œå†ç”¨queryå¯¹keyäº§ç”Ÿæƒé‡ï¼Œè¿›è¡Œattentionï¼Œå¾—åˆ°æœ€åçš„å¥å‘é‡ã€‚

        2ï¼‰LSTM+Attention

        LSTMå†…éƒ¨æœ‰Gateæœºåˆ¶ï¼Œå…¶ä¸­input gateé€‰æ‹©å“ªäº›å½“å‰ä¿¡æ¯è¿›è¡Œè¾“å…¥ï¼Œforget gateé€‰æ‹©é—å¿˜å“ªäº›è¿‡å»ä¿¡æ¯ï¼Œæˆ‘è§‰å¾—è¿™ç®—æ˜¯ä¸€å®šç¨‹åº¦çš„Attentionäº†ï¼Œè€Œä¸”å·ç§°å¯ä»¥è§£å†³é•¿æœŸä¾èµ–é—®é¢˜ï¼Œå®é™…ä¸ŠLSTMéœ€è¦ä¸€æ­¥ä¸€æ­¥å»æ•æ‰åºåˆ—ä¿¡æ¯ï¼Œåœ¨é•¿æ–‡æœ¬ä¸Šçš„è¡¨ç°æ˜¯ä¼šéšç€stepå¢åŠ è€Œæ…¢æ…¢è¡°å‡ï¼Œéš¾ä»¥ä¿ç•™å…¨éƒ¨çš„æœ‰ç”¨ä¿¡æ¯ã€‚

        LSTMé€šå¸¸éœ€è¦å¾—åˆ°ä¸€ä¸ªå‘é‡ï¼Œå†å»åšä»»åŠ¡ï¼Œå¸¸ç”¨æ–¹å¼æœ‰ï¼š

        a. ç›´æ¥ä½¿ç”¨æœ€åçš„hidden stateï¼ˆå¯èƒ½ä¼šæŸå¤±ä¸€å®šçš„å‰æ–‡ä¿¡æ¯ï¼Œéš¾ä»¥è¡¨è¾¾å…¨æ–‡ï¼‰

        b. å¯¹æ‰€æœ‰stepä¸‹çš„hidden stateè¿›è¡Œç­‰æƒå¹³å‡ï¼ˆå¯¹æ‰€æœ‰stepä¸€è§†åŒä»ï¼‰ã€‚

        c. Attentionæœºåˆ¶ï¼Œå¯¹æ‰€æœ‰stepçš„hidden stateè¿›è¡ŒåŠ æƒï¼ŒæŠŠæ³¨æ„åŠ›é›†ä¸­åˆ°æ•´æ®µæ–‡æœ¬ä¸­æ¯”è¾ƒé‡è¦çš„hidden stateä¿¡æ¯ã€‚æ€§èƒ½æ¯”å‰é¢ä¸¤ç§è¦å¥½ä¸€ç‚¹ï¼Œè€Œæ–¹ä¾¿å¯è§†åŒ–è§‚å¯Ÿå“ªäº›stepæ˜¯é‡è¦çš„ï¼Œä½†æ˜¯è¦å°å¿ƒè¿‡æ‹Ÿåˆï¼Œè€Œä¸”ä¹Ÿå¢åŠ äº†è®¡ç®—é‡ã€‚

        3ï¼‰çº¯Attention

        Attention is all you needï¼Œæ²¡æœ‰ç”¨åˆ°CNN/RNNï¼Œä¹ä¸€å¬ä¹Ÿæ˜¯ä¸€è‚¡æ¸…æµäº†ï¼Œä½†æ˜¯ä»”ç»†ä¸€çœ‹ï¼Œæœ¬è´¨ä¸Šè¿˜æ˜¯ä¸€å †å‘é‡å»è®¡ç®—attentionã€‚
    """)
    st.image("./images/attentiontypes.png")


def dl_1():
    st.image("./images/mlpipeline.png")
    st.markdown("""

        LSTM networks are a type of Recurrent Neural Network (RNN) specially designed to remember and process sequences of data over long periods. 
        What sets LSTMs apart from traditional RNNs is their ability to preserve information for long durations, courtesy of their unique structure comprising three gates: the input, forget, and output gates.
        These gates collaboratively manage the flow of information, deciding what to retain and what to discard, thereby mitigating the issue of vanishing gradients â€” a common problem in standard RNNs.
    
    """)
    
    st.header("Attention Mechanism: Enhancing LSTM")
    st.markdown("""
        The attention mechanism, initially popularized in the field of natural language processing, has found its way into various other domains, including finance. 
        It operates on a simple yet profound concept: not all parts of the input sequence are equally important. 
        By allowing the model to focus on specific parts of the input sequence while ignoring others, the attention mechanism enhances the modelâ€™s context understanding capabilities.

        Incorporating attention into LSTM networks results in a more focused and context-aware model. 
        When predicting stock prices, certain historical data points may be more relevant than others. 
        The attention mechanism empowers the LSTM to weigh these points more heavily, leading to more accurate and nuanced predictions.
    """)
    
    
    

def dl_vision():
    st.image("./images/zhang3.gif")
    st.write('''One method from deep learning that deserves the most attention for application in computer vision is: `Convolutional Neural Networks (CNNs)`.
             Additionally, both of the following network types may be useful for interpreting or developing
inference models from the features learned and extracted by CNNs; they are:
- Multilayer Perceptrons (MLP).
- Recurrent Neural Networks (RNNs).

The MLP or fully-connected type neural network layers are useful for developing models that make predictions given the learned features extracted by CNNs. RNNs, such as LSTMs,
may be helpful when working with sequences of images over time, such as with video.

Deep learning will not solve computer vision or artificial intelligence. To date, deep learning
methods have been evaluated on a broader suite of problems from computer vision and achieved
success on a small set, where success suggests performance or capability at or above what was
previously possible with other methods. Importantly, those areas where deep learning methods
are showing the greatest success are some of the more end-user facing, challenging, and perhaps
more interesting problems. Five examples include:
- Optical Character Recognition.
- Image Classification.
- Object Detection.
- Face Detection.
- Face Recognition.

All five tasks are related under the umbrella of `object recognition`, which refers to tasks that involve identifying, localizing, and/or extracting specific content from digital photographs

             ''')
    

def dl_cnn():
    st.image('./images/dl/t4.png')
    st.write('''

ä¸€ã€ä»å‰é¦ˆç¥ç»ç½‘ç»œè¯´èµ·

1.å¿…ä¼šçš„å†…åŠŸï¼šå‰é¦ˆç¥ç»ç½‘ç»œ

å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeedforward Neural Networksï¼‰æ˜¯æœ€åŸºç¡€çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œä¹Ÿè¢«ç§°ä¸ºå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ã€‚å®ƒç”±å¤šä¸ªç¥ç»å…ƒç»„æˆï¼Œæ¯ä¸ªç¥ç»å…ƒä¸å‰ä¸€å±‚çš„æ‰€æœ‰ç¥ç»å…ƒç›¸è¿ï¼Œå½¢æˆä¸€ä¸ªâ€œå…¨è¿æ¥â€çš„ç»“æ„ã€‚æ¯ä¸ªç¥ç»å…ƒä¼šå¯¹å…¶è¾“å…¥æ•°æ®è¿›è¡Œçº¿æ€§å˜æ¢ï¼ˆé€šè¿‡æƒé‡çŸ©é˜µï¼‰ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªéçº¿æ€§å‡½æ•°ï¼ˆå¦‚ReLUæˆ–Sigmoidï¼‰è¿›è¡Œæ¿€æ´»ã€‚è¿™å°±æ˜¯å‰é¦ˆç¥ç»ç½‘ç»œçš„åŸºæœ¬æ“ä½œã€‚
CNNå°±æ˜¯ä¸€ç§ç‰¹æ®Šçš„å‰é¦ˆç¥ç»ç½‘ç»œã€‚è¿™ä¸¤è€…çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼ŒCNNåœ¨å‰é¦ˆç¥ç»ç½‘ç»œçš„åŸºç¡€ä¸ŠåŠ å…¥äº†å·ç§¯å±‚å’Œæ± åŒ–å±‚ï¼ˆä¸‹è¾¹ä¼šè®²åˆ°ï¼‰ï¼Œä»¥ä¾¿æ›´å¥½åœ°å¤„ç†å›¾åƒç­‰å…·æœ‰ç©ºé—´ç»“æ„çš„æ•°æ®ã€‚

CNNå°±æ˜¯åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå°†å…¨è¿æ¥å±‚æ¢æˆå·ç§¯å±‚ï¼Œå¹¶åœ¨ReLUå±‚ä¹‹ååŠ å…¥æ± åŒ–å±‚ï¼ˆéå¿…é¡»ï¼‰ï¼Œé‚£ä¹ˆä¸€ä¸ªåŸºæœ¬çš„CNNç»“æ„å°±å¯ä»¥è¡¨ç¤ºæˆè¿™æ ·ï¼š

CNNæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºï¼Œå…¶æˆåŠŸçš„åŸå› å…³é”®åœ¨äºå®ƒæ‰€é‡‡ç”¨çš„å±€éƒ¨è¿æ¥å’Œå…±äº«æƒå€¼çš„æ–¹å¼ï¼Œä¸€æ–¹é¢å‡å°‘äº†çš„æƒå€¼çš„æ•°é‡ä½¿å¾—ç½‘ç»œæ˜“äºä¼˜åŒ–ï¼Œå¦ä¸€æ–¹é¢é™ä½äº†æ¨¡å‹å¤æ‚åº¦ï¼Œé™ä½äº†è¿‡æ‹Ÿåˆçš„é£é™©ã€‚CNNæ˜¯ä¸€ä¸ªå‰æºƒå¼ç¥ç»ç½‘ç»œï¼Œèƒ½ä»ä¸€ä¸ªäºŒç»´å›¾åƒä¸­æå–å…¶æ‹“æ‰‘ç»“æ„ï¼Œé‡‡ç”¨åå‘ä¼ æ’­ç®—æ³•æ¥ä¼˜åŒ–ç½‘ç»œç»“æ„ï¼Œæ±‚è§£ç½‘ç»œä¸­çš„æœªçŸ¥å‚æ•°ã€‚CNNå…·æœ‰ä¸€äº›ä¼ ç»ŸæŠ€æœ¯æ‰€æ²¡æœ‰çš„ä¼˜ç‚¹ï¼šè‰¯å¥½çš„å®¹é”™èƒ½åŠ›ã€å¹¶è¡Œå¤„ç†èƒ½åŠ›å’Œè‡ªå­¦ä¹ èƒ½åŠ›ï¼Œå¯å¤„ç†ç¯å¢ƒä¿¡æ¯å¤æ‚ï¼ŒèƒŒæ™¯çŸ¥è¯†ä¸æ¸…æ¥šï¼Œæ¨ç†è§„åˆ™ä¸æ˜ç¡®æƒ…å†µä¸‹çš„é—®é¢˜ï¼Œå…è®¸æ ·å“æœ‰è¾ƒå¤§çš„ç¼ºæŸã€ç•¸å˜ï¼Œè¿è¡Œé€Ÿåº¦å¿«ï¼Œè‡ªé€‚åº”æ€§èƒ½å¥½ï¼Œå…·æœ‰è¾ƒé«˜çš„åˆ†è¾¨ç‡ã€‚å®ƒæ˜¯é€šè¿‡ç»“æ„é‡ç»„å’Œå‡å°‘æƒå€¼å°†ç‰¹å¾æŠ½å–åŠŸèƒ½èåˆè¿›å¤šå±‚æ„ŸçŸ¥å™¨ï¼Œçœç•¥è¯†åˆ«å‰å¤æ‚çš„å›¾åƒç‰¹å¾æŠ½å–è¿‡ç¨‹ã€‚

CNNç½‘ç»œä¸€å…±æœ‰5ä¸ªå±‚çº§ç»“æ„ï¼š

- è¾“å…¥å±‚
- å·ç§¯å±‚
- æ¿€æ´»å±‚
- æ± åŒ–å±‚
- å…¨è¿æ¥FCå±‚
     ''')
    
def dl_mlp():
    # https://zhuanlan.zhihu.com/p/65472471
    st.image('./images/dl/t1.png')
    st.markdown("""
            #### ä»»åŠ¡æè¿°        
            æˆ‘ä»¬å·²çŸ¥å››ä¸ªæ•°æ®ç‚¹(1,1)(-1,1)(-1,-1)(1,-1)ï¼Œè¿™å››ä¸ªç‚¹åˆ†åˆ«å¯¹åº”I~IVè±¡é™ï¼Œå¦‚æœè¿™æ—¶å€™ç»™æˆ‘ä»¬ä¸€ä¸ªæ–°çš„åæ ‡ç‚¹ï¼ˆæ¯”å¦‚(2,2)ï¼‰ï¼Œé‚£ä¹ˆå®ƒåº”è¯¥å±äºå“ªä¸ªè±¡é™å‘¢ï¼Ÿï¼ˆæ²¡é”™ï¼Œå½“ç„¶æ˜¯ç¬¬Iè±¡é™ï¼Œä½†æˆ‘ä»¬çš„ä»»åŠ¡æ˜¯è¦è®©æœºå™¨çŸ¥é“ï¼‰

            â€œåˆ†ç±»â€æ˜¯ç¥ç»ç½‘ç»œçš„ä¸€å¤§åº”ç”¨ï¼Œæˆ‘ä»¬ä½¿ç”¨ç¥ç»ç½‘ç»œå®Œæˆè¿™ä¸ªåˆ†ç±»ä»»åŠ¡ã€‚        """)
    st.image('./images/dl/t2.png')
    st.markdown("""
                è¿™é‡Œæˆ‘ä»¬æ„å»ºä¸€ä¸ªä¸¤å±‚ç¥ç»ç½‘ç»œï¼Œç†è®ºä¸Šä¸¤å±‚ç¥ç»ç½‘ç»œå·²ç»å¯ä»¥æ‹Ÿåˆä»»æ„å‡½æ•°ã€‚
                1.1.è¾“å…¥å±‚

                åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œè¾“å…¥å±‚æ˜¯åæ ‡å€¼ï¼Œä¾‹å¦‚ï¼ˆ1,1ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªå…ƒç´ çš„æ•°ç»„ï¼Œä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ª1*2çš„çŸ©é˜µã€‚è¾“å…¥å±‚çš„å…ƒç´ ç»´åº¦ä¸è¾“å…¥é‡çš„ç‰¹å¾æ¯æ¯ç›¸å…³ï¼Œå¦‚æœè¾“å…¥çš„æ˜¯ä¸€å¼ 32*32åƒç´ çš„ç°åº¦å›¾åƒï¼Œé‚£ä¹ˆè¾“å…¥å±‚çš„ç»´åº¦å°±æ˜¯32*32ã€‚

                1.2.ä»è¾“å…¥å±‚åˆ°éšè—å±‚

                è¿æ¥è¾“å…¥å±‚å’Œéšè—å±‚çš„æ˜¯W1å’Œb1ã€‚ç”±Xè®¡ç®—å¾—åˆ°Hååˆ†ç®€å•ï¼Œå°±æ˜¯çŸ©é˜µè¿ç®—ï¼š


                å¦‚æœä½ å­¦è¿‡çº¿æ€§ä»£æ•°ï¼Œå¯¹è¿™ä¸ªå¼å­ä¸€å®šä¸é™Œç”Ÿã€‚å¦‚ä¸Šå›¾ä¸­æ‰€ç¤ºï¼Œåœ¨è®¾å®šéšè—å±‚ä¸º50ç»´ï¼ˆä¹Ÿå¯ä»¥ç†è§£æˆ50ä¸ªç¥ç»å…ƒï¼‰ä¹‹åï¼ŒçŸ©é˜µHçš„å¤§å°ä¸ºï¼ˆ1*50ï¼‰çš„çŸ©é˜µã€‚

                1.3.ä»éšè—å±‚åˆ°è¾“å‡ºå±‚

                è¿æ¥éšè—å±‚å’Œè¾“å‡ºå±‚çš„æ˜¯W2å’Œb2ã€‚åŒæ ·æ˜¯é€šè¿‡çŸ©é˜µè¿ç®—è¿›è¡Œçš„ï¼š                
                        
                1.4.åˆ†æ

                é€šè¿‡ä¸Šè¿°ä¸¤ä¸ªçº¿æ€§æ–¹ç¨‹çš„è®¡ç®—ï¼Œæˆ‘ä»¬å°±èƒ½å¾—åˆ°æœ€ç»ˆçš„è¾“å‡ºYäº†ï¼Œä½†æ˜¯å¦‚æœä½ è¿˜å¯¹çº¿æ€§ä»£æ•°çš„è®¡ç®—æœ‰å°è±¡çš„è¯ï¼Œåº”è¯¥ä¼šçŸ¥é“ï¼šä¸€ç³»åˆ—çº¿æ€§æ–¹ç¨‹çš„è¿ç®—æœ€ç»ˆéƒ½å¯ä»¥ç”¨ä¸€ä¸ªçº¿æ€§æ–¹ç¨‹è¡¨ç¤ºã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸Šè¿°ä¸¤ä¸ªå¼å­è”ç«‹åå¯ä»¥ç”¨ä¸€ä¸ªçº¿æ€§æ–¹ç¨‹è¡¨è¾¾ã€‚å¯¹äºä¸¤æ¬¡ç¥ç»ç½‘ç»œæ˜¯è¿™æ ·ï¼Œå°±ç®—ç½‘ç»œæ·±åº¦åŠ åˆ°100å±‚ï¼Œä¹Ÿä¾ç„¶æ˜¯è¿™æ ·ã€‚è¿™æ ·çš„è¯ç¥ç»ç½‘ç»œå°±å¤±å»äº†æ„ä¹‰ã€‚

                æ‰€ä»¥è¿™é‡Œè¦å¯¹ç½‘ç»œæ³¨å…¥çµé­‚ï¼šæ¿€æ´»å±‚ã€‚

                ##### 2.æ¿€æ´»å±‚
                ç®€è€Œè¨€ä¹‹ï¼Œæ¿€æ´»å±‚æ˜¯ä¸ºçŸ©é˜µè¿ç®—çš„ç»“æœæ·»åŠ éçº¿æ€§çš„ã€‚å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°æœ‰ä¸‰ç§ï¼Œåˆ†åˆ«æ˜¯é˜¶è·ƒå‡½æ•°ã€Sigmoidå’ŒReLUã€‚ä¸è¦è¢«å¥‡æ€ªçš„å‡½æ•°åå“åˆ°ï¼Œå…¶å®å®ƒä»¬çš„å½¢å¼éƒ½å¾ˆç®€å•ï¼Œå¦‚ä¸‹å›¾ï¼š

                å…¶ä¸­ï¼Œé˜¶è·ƒå‡½æ•°è¾“å‡ºå€¼æ˜¯è·³å˜çš„ï¼Œä¸”åªæœ‰äºŒå€¼ï¼Œè¾ƒå°‘ä½¿ç”¨ï¼›Sigmoidå‡½æ•°åœ¨å½“xçš„ç»å¯¹å€¼è¾ƒå¤§æ—¶ï¼Œæ›²çº¿çš„æ–œç‡å˜åŒ–å¾ˆå°ï¼ˆæ¢¯åº¦æ¶ˆå¤±ï¼‰ï¼Œå¹¶ä¸”è®¡ç®—è¾ƒå¤æ‚ï¼›ReLUæ˜¯å½“å‰è¾ƒä¸ºå¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ã€‚

                éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ¯ä¸ªéšè—å±‚è®¡ç®—ï¼ˆçŸ©é˜µçº¿æ€§è¿ç®—ï¼‰ä¹‹åï¼Œéƒ½éœ€è¦åŠ ä¸€å±‚æ¿€æ´»å±‚ï¼Œè¦ä¸ç„¶è¯¥å±‚çº¿æ€§è®¡ç®—æ˜¯æ²¡æœ‰æ„ä¹‰çš„ã€‚""")
    st.image('./images/dl/t3.png')
    st.markdown("""
                3.è¾“å‡ºçš„æ­£è§„åŒ–
                åœ¨å›¾4ä¸­ï¼Œè¾“å‡ºYçš„å€¼å¯èƒ½ä¼šæ˜¯(3,1,0.1,0.5)è¿™æ ·çš„çŸ©é˜µï¼Œè¯šç„¶æˆ‘ä»¬å¯ä»¥æ‰¾åˆ°é‡Œè¾¹çš„æœ€å¤§å€¼â€œ3â€ï¼Œä»è€Œæ‰¾åˆ°å¯¹åº”çš„åˆ†ç±»ä¸ºIï¼Œä½†æ˜¯è¿™å¹¶ä¸ç›´è§‚ã€‚æˆ‘ä»¬æƒ³è®©æœ€ç»ˆçš„è¾“å‡ºä¸ºæ¦‚ç‡ï¼Œä¹Ÿå°±æ˜¯è¯´å¯ä»¥ç”Ÿæˆåƒ(90%,5%,2%,3%)è¿™æ ·çš„ç»“æœï¼Œè¿™æ ·åšä¸ä»…å¯ä»¥æ‰¾åˆ°æœ€å¤§æ¦‚ç‡çš„åˆ†ç±»ï¼Œè€Œä¸”å¯ä»¥çŸ¥é“å„ä¸ªåˆ†ç±»è®¡ç®—çš„æ¦‚ç‡å€¼ã€‚

                å…·ä½“æ˜¯æ€ä¹ˆè®¡ç®—çš„å‘¢ï¼Ÿ

                è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š


                ç®€å•æ¥è¯´åˆ†ä¸‰æ­¥è¿›è¡Œï¼šï¼ˆ1ï¼‰ä»¥eä¸ºåº•å¯¹æ‰€æœ‰å…ƒç´ æ±‚æŒ‡æ•°å¹‚ï¼›ï¼ˆ2ï¼‰å°†æ‰€æœ‰æŒ‡æ•°å¹‚æ±‚å’Œï¼›ï¼ˆ3ï¼‰åˆ†åˆ«å°†è¿™äº›æŒ‡æ•°å¹‚ä¸è¯¥å’Œåšå•†ã€‚

                è¿™æ ·æ±‚å‡ºçš„ç»“æœä¸­ï¼Œæ‰€æœ‰å…ƒç´ çš„å’Œä¸€å®šä¸º1ï¼Œè€Œæ¯ä¸ªå…ƒç´ å¯ä»¥ä»£è¡¨æ¦‚ç‡å€¼ã€‚

                æˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ªè®¡ç®—å…¬å¼åšè¾“å‡ºç»“æœæ­£è§„åŒ–å¤„ç†çš„å±‚å«åšâ€œSoftmaxâ€å±‚ã€‚æ­¤æ—¶çš„ç¥ç»ç½‘ç»œå°†å˜æˆå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š


                å›¾5.è¾“å‡ºæ­£è§„åŒ–ä¹‹åçš„ç¥ç»ç½‘ç»œ
                4.å¦‚ä½•è¡¡é‡è¾“å‡ºçš„å¥½å
                é€šè¿‡Softmaxå±‚ä¹‹åï¼Œæˆ‘ä»¬å¾—åˆ°äº†Iï¼ŒIIï¼ŒIIIå’ŒIVè¿™å››ä¸ªç±»åˆ«åˆ†åˆ«å¯¹åº”çš„æ¦‚ç‡ï¼Œä½†æ˜¯è¦æ³¨æ„ï¼Œè¿™æ˜¯ç¥ç»ç½‘ç»œè®¡ç®—å¾—åˆ°çš„æ¦‚ç‡å€¼ç»“æœï¼Œè€ŒéçœŸå®çš„æƒ…å†µã€‚

                æ¯”å¦‚ï¼ŒSoftmaxè¾“å‡ºçš„ç»“æœæ˜¯(90%,5%,3%,2%)ï¼ŒçœŸå®çš„ç»“æœæ˜¯(100%,0,0,0)ã€‚è™½ç„¶è¾“å‡ºçš„ç»“æœå¯ä»¥æ­£ç¡®åˆ†ç±»ï¼Œä½†æ˜¯ä¸çœŸå®ç»“æœä¹‹é—´æ˜¯æœ‰å·®è·çš„ï¼Œä¸€ä¸ªä¼˜ç§€çš„ç½‘ç»œå¯¹ç»“æœçš„é¢„æµ‹è¦æ— é™æ¥è¿‘äº100%ï¼Œä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦å°†Softmaxè¾“å‡ºç»“æœçš„å¥½åç¨‹åº¦åšä¸€ä¸ªâ€œé‡åŒ–â€ã€‚

                ä¸€ç§ç›´è§‚çš„è§£å†³æ–¹æ³•ï¼Œæ˜¯ç”¨1å‡å»Softmaxè¾“å‡ºçš„æ¦‚ç‡ï¼Œæ¯”å¦‚1-90%=0.1ã€‚ä¸è¿‡æ›´ä¸ºå¸¸ç”¨ä¸”å·§å¦™çš„æ–¹æ³•æ˜¯ï¼Œæ±‚å¯¹æ•°çš„è´Ÿæ•°ã€‚

                è¿˜æ˜¯ç”¨90%ä¸¾ä¾‹ï¼Œå¯¹æ•°çš„è´Ÿæ•°å°±æ˜¯ï¼š-log0.9=0.046

                å¯ä»¥æƒ³è§ï¼Œæ¦‚ç‡è¶Šæ¥è¿‘100%ï¼Œè¯¥è®¡ç®—ç»“æœå€¼è¶Šæ¥è¿‘äº0ï¼Œè¯´æ˜ç»“æœè¶Šå‡†ç¡®ï¼Œè¯¥è¾“å‡ºå«åšâ€œäº¤å‰ç†µæŸå¤±ï¼ˆCross Entropy Errorï¼‰â€ã€‚

                æˆ‘ä»¬è®­ç»ƒç¥ç»ç½‘ç»œçš„ç›®çš„ï¼Œå°±æ˜¯å°½å¯èƒ½åœ°å‡å°‘è¿™ä¸ªâ€œäº¤å‰ç†µæŸå¤±â€ã€‚

                æ­¤æ—¶çš„ç½‘ç»œå¦‚ä¸‹å›¾ï¼š


                å›¾6.è®¡ç®—äº¤å‰ç†µæŸå¤±åçš„ç¥ç»ç½‘ç»œ
                5.åå‘ä¼ æ’­ä¸å‚æ•°ä¼˜åŒ–
                ä¸Šè¾¹çš„1~4èŠ‚ï¼Œè®²è¿°äº†ç¥ç»ç½‘ç»œçš„æ­£å‘ä¼ æ’­è¿‡ç¨‹ã€‚ä¸€å¥è¯å¤ä¹ ä¸€ä¸‹ï¼šç¥ç»ç½‘ç»œçš„ä¼ æ’­éƒ½æ˜¯å½¢å¦‚Y=WX+bçš„çŸ©é˜µè¿ç®—ï¼›ä¸ºäº†ç»™çŸ©é˜µè¿ç®—åŠ å…¥éçº¿æ€§ï¼Œéœ€è¦åœ¨éšè—å±‚ä¸­åŠ å…¥æ¿€æ´»å±‚ï¼›è¾“å‡ºå±‚ç»“æœéœ€è¦ç»è¿‡Softmaxå±‚å¤„ç†ä¸ºæ¦‚ç‡å€¼ï¼Œå¹¶é€šè¿‡äº¤å‰ç†µæŸå¤±æ¥é‡åŒ–å½“å‰ç½‘ç»œçš„ä¼˜åŠ£ã€‚

                ç®—å‡ºäº¤å‰ç†µæŸå¤±åï¼Œå°±è¦å¼€å§‹åå‘ä¼ æ’­äº†ã€‚å…¶å®åå‘ä¼ æ’­å°±æ˜¯ä¸€ä¸ªå‚æ•°ä¼˜åŒ–çš„è¿‡ç¨‹ï¼Œä¼˜åŒ–å¯¹è±¡å°±æ˜¯ç½‘ç»œä¸­çš„æ‰€æœ‰Wå’Œbï¼ˆå› ä¸ºå…¶ä»–æ‰€æœ‰å‚æ•°éƒ½æ˜¯ç¡®å®šçš„ï¼‰ã€‚

                ç¥ç»ç½‘ç»œçš„ç¥å¥‡ä¹‹å¤„ï¼Œå°±åœ¨äºå®ƒå¯ä»¥è‡ªåŠ¨åšWå’Œbçš„ä¼˜åŒ–ï¼Œåœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œå‚æ•°çš„æ•°é‡æœ‰æ—¶ä¼šä¸Šäº¿ï¼Œä¸è¿‡å…¶ä¼˜åŒ–çš„åŸç†å’Œæˆ‘ä»¬è¿™ä¸ªä¸¤å±‚ç¥ç»ç½‘ç»œæ˜¯ä¸€æ ·çš„ã€‚

                è¿™é‡Œä¸¾ä¸€ä¸ªå½¢è±¡çš„ä¾‹å­æè¿°ä¸€ä¸‹è¿™ä¸ªå‚æ•°ä¼˜åŒ–çš„åŸç†å’Œè¿‡ç¨‹ï¼š

                å‡è®¾æˆ‘ä»¬æ“çºµç€ä¸€ä¸ªçƒå‹æœºå™¨è¡Œèµ°åœ¨æ²™æ¼ ä¸­


                æˆ‘ä»¬åœ¨æœºå™¨ä¸­æ“çºµç€å››ä¸ªæ—‹é’®ï¼Œåˆ†åˆ«å«åšW1ï¼Œb1ï¼ŒW2ï¼Œb2ã€‚å½“æˆ‘ä»¬æ—‹è½¬å…¶ä¸­çš„æŸä¸ªæ—‹é’®æ—¶ï¼Œçƒå½¢æœºå™¨ä¼šå‘ç”Ÿç§»åŠ¨ï¼Œä½†æ˜¯æ—‹è½¬æ—‹é’®å¤§å°å’Œæœºå™¨è¿åŠ¨æ–¹å‘ä¹‹é—´çš„å¯¹åº”å…³ç³»æ˜¯ä¸çŸ¥é“çš„ã€‚è€Œæˆ‘ä»¬çš„ç›®çš„å°±æ˜¯èµ°åˆ°æ²™æ¼ çš„æœ€ä½ç‚¹ã€‚


                æ­¤æ—¶æˆ‘ä»¬è¯¥æ€ä¹ˆåŠï¼Ÿåªèƒ½æŒ¨ä¸ªè¯•å–½ã€‚

                å¦‚æœå¢å¤§W1åï¼Œçƒå‘ä¸Šèµ°äº†ï¼Œé‚£å°±å‡å°W1ã€‚

                å¦‚æœå¢å¤§b1åï¼Œçƒå‘ä¸‹èµ°äº†ï¼Œé‚£å°±ç»§ç»­å¢å¤§b1ã€‚

                å¦‚æœå¢å¤§W2åï¼Œçƒå‘ä¸‹èµ°äº†ä¸€å¤§æˆªï¼Œé‚£å°±å¤šå¢å¤§äº›W2ã€‚

                ã€‚ã€‚ã€‚

                è¿™å°±æ˜¯è¿›è¡Œå‚æ•°ä¼˜åŒ–çš„å½¢è±¡è§£é‡Šï¼ˆæœ‰æ²¡æœ‰æƒ³åˆ°æ±‚å¯¼ï¼Ÿï¼‰ï¼Œè¿™ä¸ªæ–¹æ³•å«åšæ¢¯åº¦ä¸‹é™æ³•ã€‚

                å½“æˆ‘ä»¬çš„çƒå½¢æœºå™¨èµ°åˆ°æœ€ä½ç‚¹æ—¶ï¼Œä¹Ÿå°±ä»£è¡¨ç€æˆ‘ä»¬çš„äº¤å‰ç†µæŸå¤±è¾¾åˆ°æœ€å°ï¼ˆæ¥è¿‘äº0ï¼‰ã€‚

                å…³äºåå‘ä¼ æ’­ï¼Œè¿˜æœ‰è®¸å¤šå¯ä»¥è®²çš„ï¼Œä½†æ˜¯å› ä¸ºå†…å®¹è¾ƒå¤šï¼Œå°±æ”¾åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­è¯´å§ã€‚ä¸è¿‡ä¸Šè¿°ä¾‹å­å¯¹äºç†è§£ç¥ç»ç½‘ç»œå‚æ•°ä¼˜åŒ–çš„è¿‡ç¨‹ï¼Œè¿˜æ˜¯å¾ˆæœ‰å¸®åŠ©çš„ã€‚

                6.è¿­ä»£

                ç¥ç»ç½‘ç»œéœ€è¦åå¤è¿­ä»£ã€‚

                å¦‚ä¸Šè¿°ä¾‹å­ä¸­ï¼Œç¬¬ä¸€æ¬¡è®¡ç®—å¾—åˆ°çš„æ¦‚ç‡æ˜¯90%ï¼Œäº¤å‰ç†µæŸå¤±å€¼æ˜¯0.046ï¼›å°†è¯¥æŸå¤±å€¼åå‘ä¼ æ’­ï¼Œä½¿W1,b1,W2,b2åšç›¸åº”å¾®è°ƒï¼›å†åšç¬¬äºŒæ¬¡è¿ç®—ï¼Œæ­¤æ—¶çš„æ¦‚ç‡å¯èƒ½å°±ä¼šæé«˜åˆ°92%ï¼Œç›¸åº”åœ°ï¼ŒæŸå¤±å€¼ä¹Ÿä¼šä¸‹é™ï¼Œç„¶åå†åå‘ä¼ æ’­æŸå¤±å€¼ï¼Œå¾®è°ƒå‚æ•°W1,b1,W2,b2ã€‚ä¾æ¬¡ç±»æ¨ï¼ŒæŸå¤±å€¼è¶Šæ¥è¶Šå°ï¼Œç›´åˆ°æˆ‘ä»¬æ»¡æ„ä¸ºæ­¢ã€‚

                æ­¤æ—¶æˆ‘ä»¬å°±å¾—åˆ°äº†ç†æƒ³çš„W1,b1,W2,b2ã€‚

                æ­¤æ—¶å¦‚æœå°†ä»»æ„ä¸€ç»„åæ ‡ä½œä¸ºè¾“å…¥ï¼Œåˆ©ç”¨å›¾4æˆ–å›¾5çš„æµç¨‹ï¼Œå°±èƒ½å¾—åˆ°åˆ†ç±»ç»“æœã€‚
                """)



def dl_4():
    st.markdown("""
The attention mechanism can be added to enhance the modelâ€™s ability to focus on relevant time steps:
                """)
def dl_4():
    st.markdown("""
The attention mechanism can be added to enhance the modelâ€™s ability to focus on relevant time steps:
                """)

def dl_4():
    st.markdown("""
The attention mechanism can be added to enhance the modelâ€™s ability to focus on relevant time steps:
                """)

def dl_4():
    st.markdown("""
The attention mechanism can be added to enhance the modelâ€™s ability to focus on relevant time steps:
                """)

def dl_4():
    st.markdown("""
The attention mechanism can be added to enhance the modelâ€™s ability to focus on relevant time steps:
                """)

def dl_4():
    st.markdown("""
The attention mechanism can be added to enhance the modelâ€™s ability to focus on relevant time steps:
                """)

def dl_4():
    st.markdown("""
The attention mechanism can be added to enhance the modelâ€™s ability to focus on relevant time steps:
                """)

def dl_4():
    st.markdown("""
The attention mechanism can be added to enhance the modelâ€™s ability to focus on relevant time steps:
                """)

def dl_0():
    st.markdown("""
The attention mechanism can be added to enhance the modelâ€™s ability to focus on relevant time steps:
                """)

def dl_4():
    st.markdown("""
The attention mechanism can be added to enhance the modelâ€™s ability to focus on relevant time steps:
                """)

def dl_4():
    st.markdown("""
The attention mechanism can be added to enhance the modelâ€™s ability to focus on relevant time steps:
                """)

def dl_4():
    st.markdown("""
The attention mechanism can be added to enhance the modelâ€™s ability to focus on relevant time steps:
                """)

def dl_4():
    st.markdown("""
The attention mechanism can be added to enhance the modelâ€™s ability to focus on relevant time steps:
                """)

def dl_6():
    st.markdown("""

Model: "sequential_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm_3 (LSTM)               (None, 60, 50)            10400     
                                                                 
 lstm_4 (LSTM)               (None, 60, 50)            20200     
                                                                 
 permute (Permute)           (None, 50, 60)            0         
                                                                 
 reshape (Reshape)           (None, 50, 60)            0         
                                                                 
 permute_1 (Permute)         (None, 60, 50)            0         
                                                                 
 reshape_1 (Reshape)         (None, 60, 50)            0         
                                                                 
 flatten (Flatten)           (None, 3000)              0         
                                                                 
 dense_1 (Dense)             (None, 1)                 3001      
                                                                 
 dropout (Dropout)           (None, 1)                 0         
                                                                 
 batch_normalization (Batch  (None, 1)                 4         
 Normalization)                                                  
                                                                 
=================================================================
Total params: 33605 (131.27 KB)
Trainable params: 33603 (131.26 KB)
Non-trainable params: 2 (8.00 Byte)
_________________________________________________________________


                """)

def dl_11():
    st.markdown("""
In this guide, we explored the complex yet fascinating task of using LSTM networks with an attention mechanism for stock price prediction, 
specifically for Apple Inc. (AAPL). Key points include:

- LSTMâ€™s ability to capture long-term dependencies in time-series data.
- The added advantage of the attention mechanism in focusing on relevant data points.
- The detailed process of building, training, and evaluating the LSTM model.

#### While LSTM models with attention are powerful, they have limitations:
- The assumption that historical patterns will repeat in similar ways can be problematic, especially in volatile markets.
- External factors like market news and global events, not captured in historical price data, can significantly influence stock prices.
""")

